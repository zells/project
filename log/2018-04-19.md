## 8:00

### Notes on: [The Origins of Opera and the Future of Programming][opera]

Read this very interesting article by [Jessica Kerr] yesterday. She writes about how the people were organized that ended up developing opera (this art form, not the browser) back in the 16th century.

[opera]: https://the-composition.com/the-origins-of-opera-and-the-future-of-programming-bcdaf8fbe960
[Jessica Kerr]: https://the-composition.com/@jessitron

#### Camerata

The groups were called *Camerata* (small orchestra/choir) which was a highly diverse group consisting of "musicians, artists, poets, astrologers, philosophers and scientists", united by a common problem: "they were bored with polyphony, the esteemed music of their day" and "asked the revolutionary question: what if you could understand the words?"

I feel like it would probably need a group like this to advance Software Literacy. Or anyways I personally would love to be part of such a group. The more diverse, the better. Solutions that programmers find will always be solutions for programmers. But what would the common problem be? How about: **Must Software be so difficult? How can we make it easier to write, and (maybe most importantly) to read Software?"

Jessica also writes about how the surrounding culture must be *ripe*. I think this might very well be the case right now, with privacy, security and data ownership becoming more of a topic. If you want to really own your data, you must be Software Literate.

#### Symmathesy

The major conclusion of the article is that "Software is not a Craft.", "Software is not an Art.", "Software is the next thing after art" which she calls *Symmathesy* (*Sym* = together, *mathesy* = learning). I always felt that software development was more like gardening. But I decided I need to learn some more about *System Thinking* and *Symmathesy*.

Another nice concept in the article is *Gnerativity*, which "is the difference between the teamâ€™s output with me and without me". Again, the group is the important entity, not the individual.

#### Mental Models

But the part that struck the loudest chord with me was about mental models. For a couple of years, I started to emphasize the Model in my software projects, mostly by adopting the practices of Domain-Driven Design. Because Software Development, in the end, is really externalizing a mental model. Or more exactly *synchronzing your mental model with the implemented model". And a lot of working in a team is *synchronzing the mental models of each team member". Both activities yield a [Coherence Penalty].

So I really like Jessica's model where **the system is part of the team** with the main activities being **learning** and **synchronizing models** (the mental one of each human, and the implemented one in the system).

Jessica proposes that one problem with software is that it is much easier, to build a new mental model and implement it than to understand the model that somebody else has implemented. The effect is a thousand JavaScript frameworks. I myself am very much victim of the "not invented here" syndrome. So I'm convinced that there is a lot of truth in this.

All this talk about models made me thinking again about what *Literacy* means. It's not enough to just read or write. The important part is to *understand* what is written and *externalize your own ideas*. In other words, if two people are literate in a specific medium, then it's possible to transfer the mental model of one person's brain to the other person's brain, through the medium, be it text, math, drawings or music. It's all about synchronizing mental models.

What makes software qualitatively different from all other media, is that a model expressed in it does not need a human interpreter to **do** something.

To enable Software Literacy, we need to make it easier to externalize your own mental models into software. But maybe even more importantly, we need to make it much easier to internalize the model of a piece of software. You can just *run* the software, sure. But for better insight, or if something unexpected happens, or if you want to improve the model, you need to be able to easily create a mental model, that is as close as possible to the implemented one. This is what *reading software* means. And this is where current environments fails miserably, where "new frameworks per week" could probably be used to measure how miserably.

Because reading software is so difficult, we mostly rely on a prose description of the author's mental model to do so, called "documentation" and "comments". But there is always the chance, that this model is badly out-of-sync with the implemented one, made more probable by the extra work that externalizing your model twice requires.

I think one of the main problems is that we are expected to understand the implemented model by analyzing static text (aka programm code). But since it's a *dynamic* model, it's usually way easier to analyize it dynamically (aka *play around* with it). It should be very easy and fast to execute the model and see how it behaves when some of its parts are changed. Because most programming environments lack a truely interactive way of analyzing the programm, I mostly rely on automated tests to describe the implemented model in a dynamic way that can be easily experimented with. But for me (and probably anyone who programmed in Smalltalk), this feels like a crutch.

[Coherence Penalty]: http://www.michaelnygard.com/blog/2018/01/coherence-penalty-for-humans/